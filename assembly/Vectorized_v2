#define STDOUT 0xd0580000

.section .text
.global _start
_start:
    # Scalar Bit-Reversal (from your working non-vectorized code)
    la t0, real
    la t1, imag
    la t2, bitrev_indices
    la t3, rev_real
    la t4, rev_imag
    li t5, 0
bitrev_loop:
    li t6, 128
    bge t5, t6, end_bitrev
    slli s4, t5, 2
    add s5, t2, s4
    lw s6, 0(s5)
    slli s7, s6, 2
    add s8, t0, s4
    add s9, t1, s4
    flw ft0, 0(s8)
    flw ft1, 0(s9)
    add s8, t3, s7
    add s9, t4, s7
    fsw ft0, 0(s8)
    fsw ft1, 0(s9)
    addi t5, t5, 1
    j bitrev_loop
end_bitrev:

    # ----------------------------- FFT SETUP -----------------------------
    li t6, 7   # Max stages (s4 will go from 1 to t6)
    la s0, rev_real   # base address of rev_real[]
    la s1, rev_imag   # base address of rev_imag[]
    la s2, twiddle_real # base address of twiddle_real[]
    la s3, twiddle_imag # base address of twiddle_imag[]
    li s4, 1   # s4 = current FFT stage number (1 to 7)

    # ----------------------------- FFT COMPUTATION -----------------------------
outer_stage_loop:
    bgt s4, t6, end_fft_stages  # If stage > max_stages, done

    li s5, 1
    sll s5, s5, s4          # s5 = m = 2^stage (group size for this FFT stage)
    srli s6, s5, 1          # s6 = half_m = m/2 (number of butterflies in a group)

    li s7, 128              # s7 = N (total number of points)
    div s8, s7, s5          # s8 = twiddle_stride = N / m

    li t0, 0                # t0 = i = 0 (outer group index, iterates by m)

outer_i_loop:
    bge t0, s7, end_outer_i_loop # if i >= N, end this stage's groups

    mv s9, x0                   # s9 = j_base = 0 (start of j for current vector strip)

vectorized_j_loop:              # This loop processes butterflies in vector strips
    sub s10, s6, s9             # s10 = remaining_j_in_group = half_m - j_base

    mv t1, x0                   # Clear t1 (destination for vl) before vsetvli as a precaution
    vsetvli t1, s10, e32, m1, ta, ma # t1 = actual vl for this iteration. AVL is remaining_j.
    beqz t1, end_vectorized_j_loop # If vl (t1) is 0, all j's for this i-group are done.

    add t2, t0, s9              # t2 = i + j_base (scalar part of index)
    slli t2, t2, 2              # t2 = byte_offset_for_first_X_top

    add t3, t0, s9              # t3 = i + j_base
    add t3, t3, s6              # t3 = i + j_base + m/2 (scalar part of index)
    slli t3, t3, 2              # t3 = byte_offset_for_first_X_bottom

    add t5, s0, t2
    vle32.v v0, (t5)
    add t5, s1, t2
    vle32.v v1, (t5)

    add t5, s0, t3
    vle32.v v2, (t5)
    add t5, s1, t3
    vle32.v v3, (t5)

    mul s10, s9, s8
    # csrr t4, vstart # Save vstart - using t4, ensure it's not critical here
    # csrw vstart, x0 # Ensure vstart is 0 for vid.v
    vid.v v20
    # csrw vstart, t4 # Restore vstart

    vmul.vx v20, v20, s8
    vadd.vx v20, v20, s10

    # Corrected: vslli.vi v21, v20, 2  ->  li t4, 4; vmul.vx v21, v20, t4
    li t4, 4                    # Load scalar 4 into a temporary GPR (t4)
    vmul.vx v21, v20, t4        # v21 = v20 * 4 (byte offsets)

    vluxei32.v v4, (s2), v21
    vluxei32.v v5, (s3), v21

    vfmul.vv v6, v2, v4
    vfmul.vv v7, v3, v5
    vfsub.vv v8, v6, v7

    vfmul.vv v6, v2, v5
    vfmul.vv v7, v3, v4
    vfadd.vv v9, v6, v7

    vfadd.vv v10, v0, v8
    vfadd.vv v11, v1, v9
    vfsub.vv v12, v0, v8
    vfsub.vv v13, v1, v9

    add t5, s0, t2
    vse32.v v10, (t5)
    add t5, s1, t2
    vse32.v v11, (t5)

    add t5, s0, t3
    vse32.v v12, (t5)
    add t5, s1, t3
    vse32.v v13, (t5)

    add s9, s9, t1
    j vectorized_j_loop
end_vectorized_j_loop:

    # Corrected: addi t0, t0, s5  ->  add t0, t0, s5
    add t0, t0, s5              # i += m (s5 is m)
    j outer_i_loop
end_outer_i_loop:

    addi s4, s4, 1              # next stage
    j outer_stage_loop
end_fft_stages:

    # --- Final Output ---
    la a0, filename_real
    mv a1, s0
    li a2, 128 * 4
    call write_to_file

    la a0, filename_imag
    mv a1, s1
    li a2, 128 * 4
    call write_to_file

    j _finish

# ----------------------------- write_to_file Function -----------------------------
write_to_file:
    addi sp, sp, -16
    sw ra, 0(sp)
    sw s0, 4(sp)
    sw s1, 8(sp)
    sw s2, 12(sp)
    mv s1, a1
    mv s2, a2
    li a1, 0x601
    li a2, 0x1b6
    call open
    mv s0, a0
    mv a0, s0
    mv a1, s1
    mv a2, s2
    call write
    mv a0, s0
    call close
    lw ra, 0(sp)
    lw s0, 4(sp)
    lw s1, 8(sp)
    lw s2, 12(sp)
    addi sp, sp, 16
    ret

# ----------------------------- _finish Function -------------------------------------
_finish:
    li x3, 0xd0580000
    li t5, 1 # Standard success code for tohost often involves 1
    slli t5, t5, 1
    ori  t5, t5, 1
    mv   x5, t5
    sb x5, 0(x3)
_final_halt_loop:
    beq x0, x0, _final_halt_loop # Halt
    .rept 100
        nop
    .endr

# ----------------------------- Data Section -----------------------------
.section .data
.align 4

# PADDING FOR STRING ALIGNMENT IS CRUCIAL
# These debug filenames are not strictly needed for the full run but kept for consistency
filename_vl_debug: .string "vl_output.hex"
.byte 0,0                 # Pad to 16 bytes (14 + 2)

.align 4
vl_debug_value: .word 0xA5A5A5A5

.align 4
filename_rev_real0: .string "rev_real0_output.hex"
.byte 0,0,0               # Pad to 24 bytes (21 + 3)

.align 4
filename_real: .string "output_real.hex"    # 16 chars + 1 null = 17 bytes
.byte 0,0,0               # Pad to 20 bytes (17 + 3 = 20, which is 4-byte aligned)

.align 4
filename_imag: .string "output_imag.hex"    # 16 chars + 1 null = 17 bytes
.byte 0,0,0               # Pad to 20 bytes

.align 4
real:
.float 0.00000000, 0.38268343, 0.70710678, 0.92387953, 1.00000000, 0.92387953, 0.70710678, 0.38268343
.float 0.00000000, -0.38268343, -0.70710678, -0.92387953, -1.00000000, -0.92387953, -0.70710678, -0.38268343
.float -0.00000000, 0.38268343, 0.70710678, 0.92387953, 1.00000000, 0.92387953, 0.70710678, 0.38268343
.float 0.00000000, -0.38268343, -0.70710678, -0.92387953, -1.00000000, -0.92387953, -0.70710678, -0.38268343
.float -0.00000000, 0.38268343, 0.70710678, 0.92387953, 1.00000000, 0.92387953, 0.70710678, 0.38268343
.float 0.00000000, -0.38268343, -0.70710678, -0.92387953, -1.00000000, -0.92387953, -0.70710678, -0.38268343
.float -0.00000000, 0.38268343, 0.70710678, 0.92387953, 1.00000000, 0.92387953, 0.70710678, 0.38268343
.float 0.00000000, -0.38268343, -0.70710678, -0.92387953, -1.00000000, -0.92387953, -0.70710678, -0.38268343
.float -0.00000000, 0.38268343, 0.70710678, 0.92387953, 1.00000000, 0.92387953, 0.70710678, 0.38268343
.float 0.00000000, -0.38268343, -0.70710678, -0.92387953, -1.00000000, -0.92387953, -0.70710678, -0.38268343
.float -0.00000000, 0.38268343, 0.70710678, 0.92387953, 1.00000000, 0.92387953, 0.70710678, 0.38268343
.float 0.00000000, -0.38268343, -0.70710678, -0.92387953, -1.00000000, -0.92387953, -0.70710678, -0.38268343
.float -0.00000000, 0.38268343, 0.70710678, 0.92387953, 1.00000000, 0.92387953, 0.70710678, 0.38268343
.float 0.00000000, -0.38268343, -0.70710678, -0.92387953, -1.00000000, -0.92387953, -0.70710678, -0.38268343
.float -0.00000000, 0.38268343, 0.70710678, 0.92387953, 1.00000000, 0.92387953, 0.70710678, 0.38268343
.float 0.00000000, -0.38268343, -0.70710678, -0.92387953, -1.00000000, -0.92387953, -0.70710678, -0.38268343

.align 4
imag:
.rept 128
    .float 0.0
.endr

.align 4
rev_real: .space 512
.align 4
rev_imag: .space 512

.align 4
bitrev_indices:
.word 0, 64, 32, 96, 16, 80, 48, 112, 8, 72, 40, 104, 24, 88, 56, 120, 4, 68, 36, 100, 20, 84, 52, 116, 12, 76, 44, 108, 28, 92, 60, 124, 2, 66, 34, 98, 18, 82, 50, 114, 10, 74, 42, 106, 26, 90, 58, 122, 6, 70, 38, 102, 22, 86, 54, 118, 14, 78, 46, 110, 30, 94, 62, 126, 1, 65, 33, 97, 17, 81, 49, 113, 9, 73, 41, 105, 25, 89, 57, 121, 5, 69, 37, 101, 21, 85, 53, 117, 13, 77, 45, 109, 29, 93, 61, 125, 3, 67, 35, 99, 19, 83, 51, 115, 11, 75, 43, 107, 27, 91, 59, 123, 7, 71, 39, 103, 23, 87, 55, 119, 15, 79, 47, 111, 31, 95, 63, 127

.align 4
twiddle_real:
.float 1.00000000, 0.99879546, 0.99518473, 0.98917651, 0.98078528, 0.97003125, 0.95694034, 0.94154407, 0.92387953, 0.90398929, 0.88192126, 0.85772861, 0.83146961, 0.80320753, 0.77301045, 0.74095113, 0.70710678, 0.67155895, 0.63439328, 0.59569930, 0.55557023, 0.51410274, 0.47139674, 0.42755509, 0.38268343, 0.33688985, 0.29028468, 0.24298018, 0.19509032, 0.14673047, 0.09801714, 0.04906767, 0.00000000, -0.04906767, -0.09801714, -0.14673047, -0.19509032, -0.24298018, -0.29028468, -0.33688985, -0.38268343, -0.42755509, -0.47139674, -0.51410274, -0.55557023, -0.59569930, -0.63439328, -0.67155895, -0.70710678, -0.74095113, -0.77301045, -0.80320753, -0.83146961, -0.85772861, -0.88192126, -0.90398929, -0.92387953, -0.94154407, -0.95694034, -0.97003125, -0.98078528, -0.98917651, -0.99518473, -0.99879546

.align 4
twiddle_imag:
.float -0.00000000, -0.04906767, -0.09801714, -0.14673047, -0.19509032, -0.24298018, -0.29028468, -0.33688985, -0.38268343, -0.42755509, -0.47139674, -0.51410274, -0.55557023, -0.59569930, -0.63439328, -0.67155895, -0.70710678, -0.74095113, -0.77301045, -0.80320753, -0.83146961, -0.85772861, -0.88192126, -0.90398929, -0.92387953, -0.94154407, -0.95694034, -0.97003125, -0.98078528, -0.98917651, -0.99518473, -0.99879546, -1.00000000, -0.99879546, -0.99518473, -0.98917651, -0.98078528, -0.97003125, -0.95694034, -0.94154407, -0.92387953, -0.90398929, -0.88192126, -0.85772861, -0.83146961, -0.80320753, -0.77301045, -0.74095113, -0.70710678, -0.67155895, -0.63439328, -0.59569930, -0.55557023, -0.51410274, -0.47139674, -0.42755509, -0.38268343, -0.33688985, -0.29028468, -0.24298018, -0.19509032, -0.14673047, -0.09801714, -0.04906767

.align 4
size: .word 128